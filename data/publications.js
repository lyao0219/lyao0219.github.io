import {projectLinkIcon, publicationAwardType, publicationTypeList} from "./variables.js";

export const bibtex = {
    1: "@article{Yao:2024:SVIM,\n" +
        "  TITLE = {{Situated Visualization in Motion}},\n" +
        "  AUTHOR = {Yao, Lijie},\n" +
        "  URL = {https://hal.science/hal-04700820},\n" +
        "  JOURNAL = {{IEEE Computer Graphics and Applications}},\n" +
        "  PUBLISHER = {{Institute of Electrical and Electronics Engineers}},\n" +
        "  YEAR = {2024},\n" +
        "  volume={44}, \n" +
        "  number={6}, \n" +
        "  pages={142-150}, \n" +
        "  DOI = {10.1109/MCG.2024.3462129},\n" +
        "  KEYWORDS = {Situated visualization ; Visualization in motion ; Visualization design and evaluation methods},\n" +
        "  PDF = {https://lijieyao.com/assets/pdf/Situated_Visualization_in_Motion.pdf},\n" +
        "}\n",
    2: "@article{Yao:2025:VideoGame,\n" +
        "  TITLE = {{User Experience of Visualizations in Motion: A Case Study and Design Considerations}},\n" +
        "  AUTHOR = {Yao, Lijie and Bucchieri, Federica and Mcarthur, Victoria and Bezerianos, Anastasia and Isenberg, Petra},\n" +
        "  URL = {https://doi.org/10.1109/tvcg.2024.3456319},\n" +
        "  JOURNAL = {{IEEE Transactions on Visualization and Computer Graphics}},\n" +
        "  PUBLISHER = {{Institute of Electrical and Electronics Engineers}},\n" +
        "  VOLUME = {31},\n" +
        "  NUMBER = {1},\n" +
        "  PAGES = {174--184},\n" +
        "  YEAR = {2025},\n" +
        "  MONTH = Jan,\n" +
        "  DOI = {10.1109/tvcg.2024.3456319},\n" +
        "  KEYWORDS = {Situated visualization ; Visualization in motion ; Design considerations},\n" +
        "  PDF = {https://lijieyao.com/assets/pdf/User_Experience_of_Visualizations_in_Motion_A_Case_Study_and_Design_Considerations.pdf},\n" +
        "}\n",
    3: "@article{Yao:2024:Swimming,\n" +
        "  TITLE = {{Designing for Visualization in Motion: Embedding Visualizations in Swimming Videos}},\n" +
        "  AUTHOR = {Yao, Lijie and Vuillemot, Romain and Bezerianos, Anastasia and Isenberg, Petra},\n" +
        "  URL = {https://doi.org/10.1109/tvcg.2023.334},\n" +
        "  JOURNAL = {{IEEE Transactions on Visualization and Computer Graphics}},\n" +
        "  PUBLISHER = {{Institute of Electrical and Electronics Engineers}},\n" +
        "  VOLUME = {30},\n" +
        "  NUMBER = {3},\n" +
        "  PAGES = {1821-1836},\n" +
        "  YEAR = {2024},\n" +
        "  MONTH = Mar,\n" +
        "  DOI = {10.1109/tvcg.2023.3341990},\n" +
        "  KEYWORDS = {Embedded visualization ; sports analytics ; design framework ; visualization in motion},\n" +
        "  PDF = {https://lijieyao.com/assets/pdf/Designing_for_Visualization_in_Motion_Embedding_Visualizations_in_Swimming_Videos.pdf},\n" +
        "}",
    4: "@article{Yao:2022:VIM,\n" +
        "  TITLE = {{Visualization in Motion: A Research Agenda and Two Evaluations}},\n" +
        "  AUTHOR = {Yao, Lijie and Bezerianos, Anastasia and Vuillemot, Romain and Isenberg, Petra},\n" +
        "  URL = {https://doi.org/10.1109/TVCG.2022.3184993},\n" +
        "  JOURNAL = {{IEEE Transactions on Visualization and Computer Graphics}},\n" +
        "  PUBLISHER = {{Institute of Electrical and Electronics Engineers}},\n" +
        "  VOLUME = {28},\n" +
        "  NUMBER = {10},\n" +
        "  PAGES = {3546-3562},\n" +
        "  YEAR = {2022},\n" +
        "  MONTH = Oct,\n" +
        "  DOI = {10.1109/TVCG.2022.3184993},\n" +
        "  KEYWORDS = {Visualization ; Visualization in motion ; Perception ; Research agenda ; Movement ; Motion},\n" +
        "  PDF = {https://lijieyao.com/assets/pdf/Vis_in_motion_A_Research_Agenda_and_Two_Evaluations.pdf},\n" +
        "}",
    5: "@inproceedings{Grioui:2024:SmartWatch,\n" +
        "  TITLE = {{Micro Visualizations on a Smartwatch: Assessing Reading Performance While Walking}},\n" +
        "  AUTHOR = {Grioui, Fairouz and Blascheck, Tanja and Yao, Lijie and Isenberg, Petra},\n" +
        "  URL = {https://doi.org/10.1109/VIS55277.2024.00017},\n" +
        "  NOTE = {IEEE Visualization and Visual Analytics (VIS)},\n" +
        "  BOOKTITLE = {{IEEE Visualization & Visual Analytics (IEEE VIS)}},\n" +
        "  ADDRESS = {Los Alamitos, CA, USA, United States},\n" +
        "  YEAR = {2024},\n" +
        "  MONTH = Oct,\n" +
        "  DOI = {10.1109/VIS55277.2024.00017},\n" +
        "  KEYWORDS = {micro and mobile visualization ; smartwatch ; visualization},\n" +
        "  PDF = {https://lijieyao.com/assets/pdf/Micro_Visualizations_on_a_Smartwatch_Assessing_Reading_Performance_While_Walking.pdf},\n" +
        "}",
    6: "@inproceedings{Elshabasi:2024:FirstPerson,\n" +
        "  TITLE = {{Collecting Information Needs for Egocentric Visualizations while Running}},\n" +
        "  AUTHOR = {Elshabasi, Ahmed and Yao, Lijie and Isenberg, Petra and Perin, Charles and Willett, Wesley},\n" +
        "  URL = {https://hal.science/hal-04700822},\n" +
        "  BOOKTITLE = {{IEEE VIS Workshop on First-Person Visualizations for Outdoor Physical Activities: Challenges and Opportunities}},\n" +
        "  ADDRESS = {St. Pete Beach (Florida), United States},\n" +
        "  YEAR = {2024},\n" +
        "  MONTH = Oct,\n" +
        "  DOI = {10.48550/arXiv.2409.06426},\n" +
        "  KEYWORDS = {Situated visualization ; Visualization in motion ; Sports},\n" +
        "  PDF = {https://lijieyao.com/assets/pdf/Collecting_Information_Needs_for_Egocentric_Visualizations_while_Running.pdf},\n" +
        "}",
    7: "@inproceedings{Jansen:2022:SV,\n" +
        "  TITLE = {{Envisioning Situated Visualizations of Environmental Footprints in an Urban Environment}},\n" +
        "  AUTHOR = {Jansen, Yvonne and Bucchieri, Federica and Dragicevic, Pierre and Hachet, Martin and Koval, Morgane and Petiot, L{\\'e}ana and Prouzeau, Arnaud and Schmalstieg, Dieter and Yao, Lijie and Isenberg, Petra},\n" +
        "  URL = {https://inria.hal.science/hal-03770857},\n" +
        "  BOOKTITLE = {{IEEE VIS Workshop on Visualization for Social Good (VIS4Good)}},\n" +
        "  ADDRESS = {Oklahoma City, United States},\n" +
        "  YEAR = {2022},\n" +
        "  MONTH = Oct,\n" +
        "  DOI = {10.5281/zenodo.7053934},\n" +
        "  KEYWORDS = {Situated visualization ; environmental footprint ; workshop ; ideation exercise},\n" +
        "  PDF = {https://lijieyao.com/assets/pdf/Environment_Footprint_by_Situated_Visualization.pdf},\n" +
        "}",
    8: "@inproceedings{Islam:2022:FitnessTracker,\n" +
        "  TITLE = {{Reflections on Visualization in Motion for Fitness Trackers}},\n" +
        "  AUTHOR = {Islam, Alaul and Yao, Lijie and Bezerianos, Anastasia and Blascheck, Tanja and He, Tingying and Lee, Bongshin and Vuillemot, Romain and Isenberg, Petra},\n" +
        "  URL = {https://inria.hal.science/hal-03775633},\n" +
        "  BOOKTITLE = {{MobileHCI Workshop on New Trends in HCI and Sports (NTSPORT)}},\n" +
        "  ADDRESS = {Vancouver, Canada},\n" +
        "  YEAR = {2022},\n" +
        "  MONTH = Sep,\n" +
        "  DOI = {10.48550/arXiv.2409.06401},\n" +
        "  KEYWORDS = {Fitness Trackers ; Visualization in Motion ; Sports Analytics ; Wearable Devices},\n" +
        "  PDF = {https://lijieyao.com/assets/pdf/Relfections_on_Visualization_in_Motion_for_Fitness_Trackers.pdf},\n" +
        "}",
    9: "@inproceedings{Bucchieri:2022:VideoGame,\n" +
        "  TITLE = {{Situated Visualization in Motion for Video Games}},\n" +
        "  AUTHOR = {Bucchieri, Federica and Yao, Lijie and Isenberg, Petra},\n" +
        "  URL = {https://inria.hal.science/hal-03694019},\n" +
        "  NOTE = {Poster},\n" +
        "  HOWPUBLISHED = {{Posters of the European Conference on Visualization (EuroVis)}},\n" +
        "  YEAR = {2022},\n" +
        "  MONTH = Jun,\n" +
        "  DOI = {10.2312/evp.20221119},\n" +
        "  KEYWORDS = {Visualization ; Visualization in motion ; Visualization for video games ; Situated visualization},\n" +
        "  PDF = {https://lijieyao.com/assets/pdf/Situated_Visualization_in_Motion_for_Video_Games.pdf},\n" +
        "}",
    10: "@inproceedings{Yao:2022:Swimming,\n" +
        "  TITLE = {{Situated Visualization in Motion for Swimming}},\n" +
        "  AUTHOR = {Yao, Lijie and Bezerianos, Anastasia and Vuillemot, Romain and Isenberg, Petra},\n" +
        "  URL = {https://inria.hal.science/hal-03700406},\n" +
        "  BOOKTITLE = {{Journ{\\'e}e Visu 2022}},\n" +
        "  YEAR = {2022},\n" +
        "  MONTH = Jun,\n" +
        "  DOI = {10.48550/arXiv.2409.07695},\n" +
        "  PDF = {https://lijieyao.com/assets/pdf/Situated_Visualization_in_Motion_for_Swimming.pdf},\n" +
        "}",
    11: "@inproceedings{Bucchieri:2022:DataType,\n" +
        "  TITLE = {{Visualization in Motion in Video Games for Different Types of Data}},\n" +
        "  AUTHOR = {Bucchieri, Federica and Yao, Lijie and Isenberg, Petra},\n" +
        "  URL = {https://inria.hal.science/hal-03700418},\n" +
        "  BOOKTITLE = {{Journ{\\'e}e Visu 2022}},\n" +
        "  ADDRESS = {Bordeaux, France},\n" +
        "  YEAR = {2022},\n" +
        "  MONTH = Jun,\n" +
        "  DOI = {10.48550/arXiv.2409.07696},\n" +
        "  PDF = {https://lijieyao.com/assets/pdf/Visualization_in_Motion_for_Video_Games_Data_Type.pdf},\n" +
        "}",
    12: "@inproceedings{Yao:2020:VIM,\n" +
        "  TITLE = {{Situated Visualization in Motion}},\n" +
        "  AUTHOR = {Yao, Lijie and Bezerianos, Anastasia and Isenberg, Petra},\n" +
        "  URL = {https://inria.hal.science/hal-02946587},\n" +
        "  NOTE = {Poster},\n" +
        "  HOWPUBLISHED = {{Posters of the IEEE Conference on Visualization}},\n" +
        "  YEAR = {2020},\n" +
        "  MONTH = Oct,\n" +
        "  DOI = {10.48550/arXiv.2409.07005},\n" +
        "  PDF = {https://lijieyao.com/assets/pdf/Situated_Visualization_in_Motion_Poster.pdf},\n" +
        "}",
    13: "@phdthesis{Yao:PhDThesis,\n" +
        "  TITLE = {{Situated Visualization in Motion}},\n" +
        "  AUTHOR = {Yao, Lijie},\n" +
        "  URL = {https://theses.hal.science/tel-04413122},\n" +
        "  NUMBER = {2023UPASG093},\n" +
        "  SCHOOL = {{Universit{\'e} Paris-Saclay}},\n" +
        "  YEAR = {2023},\n" +
        "  MONTH = Dec,\n" +
        "  KEYWORDS = {Situated visualization ; Embedded visualization ; Visualization design ; Perception ; Sports analytics ; Video games},\n" +
        "  TYPE = {Theses},\n" +
        "  PDF = {https://theses.hal.science/tel-04413122v1/file/129344_YAO_2023_archivage.pdf},\n" +
        "}",
    14: "@inproceedings{Yao:2025:PhysicalVisWidget,\n" +
        "  TITLE = {{Designing Visualization Widgets for Tangible Data Exploration: A Systematic Review}},\n" +
        "  AUTHOR = {Yao, Haonan and Yu, Lingyun and Yao, Lijie},\n" +
        "  URL = {https://physicalviswidget.github.io/},\n" +
        "  NOTE = {To appear},\n" +
        "  BOOKTITLE = {{IEEE Visualization & Visual Analytics (IEEE VIS)}},\n" +
        "  ADDRESS = {Los Alamitos, CA, USA, United States},\n" +
        "  YEAR = {2025},\n" +
        "  MONTH = Nov,\n" +
        "  DOI = {10.48550/arXiv.2507.00775},\n" +
        "  KEYWORDS = {visualization widget, tangible interaction, data exploration},\n" +
        "  PDF = {https://lijieyao.com/assets/pdf/Designing_Visualization_Widgets_for_Tangible_Data_Exploration_A_Systematic_Review.pdf},\n" +
        "}",
    15: "@inproceedings{Yao:2025:VIM,\n" +
        "  TITLE = {{Visualization in Motion: Perception, Design, and User Experience}},\n" +
        "  AUTHOR = {Yao, Lijie},\n" +
        "  NOTE = {Poster},\n" +
        "  HOWPUBLISHED = {{Posters of ChinaVis}},\n" +
        "  YEAR = {2025},\n" +
        "  MONTH = July,\n" +
        "  PDF = {https://lijieyao.com/assets/pdf/Poster_VisInMotion.pdf},\n" +
        "}",
    16: "@inproceedings{Qi:2025:EarlyExploration,\n" +
        "  TITLE = {{Early Exploration into AI-Assisted Visual Analytics for Dynamic Videos}},\n" +
        "  AUTHOR = {Qi, Guoy and Li, Junyi and Hong, Jiayi and Yao, Lijie},\n" +
        "  BOOKTITLE = {{IEEE VIS Workshop on GenAI, Agents, and the Future of VIS}},\n" +
        "  ADDRESS = {Vienna, Austria},\n" +
        "  YEAR = {2025},\n" +
        "  MONTH = Nov,\n" +
        "  NOTE = {To appear},\n" +
        "  DOI = {},\n" +
        "  KEYWORDS = {visualization, video analysis, large language model},\n" +
        "  PDF = {https://lijieyao.com/assets/pdf/Early_Exploration_into_AI-Assisted_Visual_Analytics_for_Dynamic_Videos.pdf},\n" +
        "}",
    17: "@inproceedings{Zhao:2025:StructuredAgents,\n" +
        "  TITLE = {{Structured AI Agents for Reliable Visualization Report Generation}},\n" +
        "  AUTHOR = {Zhao, Junhao and Yao, Lijie},\n" +
        "  BOOKTITLE = {{IEEE VIS Workshop on GenAI, Agents, and the Future of VIS}},\n" +
        "  ADDRESS = {Vienna, Austria},\n" +
        "  YEAR = {2025},\n" +
        "  MONTH = Nov,\n" +
        "  NOTE = {To appear},\n" +
        "  DOI = {},\n" +
        "  KEYWORDS = {structured AI agents, reliable visualization, hallucination reduction},\n" +
        "  PDF = {https://lijieyao.com/assets/pdf/Structured_AI_Agents_for_Reliable_Visualization_Report_Generation.pdf},\n" +
        "}",
    18: "@inproceedings{Yao:2025:InputPhysicalization,\n" +
        "  TITLE = {{Input Physicalization in Practice: An Instructional Walkthrough with Visualization Novices}},\n" +
        "  AUTHOR = {Yao, Lijie and Zhang, Yuming and Liu, Yu},\n" +
        "  BOOKTITLE = {{IEEE VIS Workshop on Input Visualization}},\n" +
        "  ADDRESS = {Vienna, Austria},\n" +
        "  YEAR = {2025},\n" +
        "  MONTH = Nov,\n" +
        "  NOTE = {To appear},\n" +
        "  DOI = {},\n" +
        "  KEYWORDS = {physicalization, personal data, design walkthrough},\n" +
        "  PDF = {https://lijieyao.com/assets/pdf/Input_Physicalization_in_Practice_An_Instructional_Walkthrough_with_Visualization_Novices.pdf},\n" +
        "}",
    19: "@inproceedings{Xu:2025:TouchSoundSpace,\n" +
        "  TITLE = {{Touch, Sound, and Space: Exploring Immersive Music Interaction through AI-Generated Environments}},\n" +
        "  AUTHOR = {Xu, Wanfang and Yang, Jifan and Zhang, Fengwen and Lu, Yu and Yao, Lijie and Liu, Le and Yu, Lingyun},\n" +
        "  BOOKTITLE = {{International Symposium on Visual Information Communication and Interaction (VINCI)}},\n" +
        "  ADDRESS = {Linz, Austria},\n" +
        "  YEAR = {2025},\n" +
        "  MONTH = Dec,\n" +
        "  NOTE = {To appear},\n" +
        "  DOI = {},\n" +
        "  KEYWORDS = {immersive creation, music generation, visual design},\n" +
        "  PDF = {https://lijieyao.com/assets/pdf/Touch_Sound_and_Space_Exploring_Immersive_Music_Interaction_through_AI-Generated_Environments.pdf},\n" +
        "}",

}


export const publications = [
    {
        id: 1,
        event: "IEEE VIS",
        type: publicationTypeList.ShortPaper,
        title: "Designing Visualization Widgets for Tangible Data Exploration: A Systematic Review",
        author: "Haonan Yao, Lingyun Yu, <strong>Lijie Yao*</strong>",
        subtitle: "IEEE Visualization & Visual Analytics (IEEE VIS), November 2025, Vienna, Austria, To appear.",
        teaser: "assets/teaser/physicalwidgetshort.jpg",
        year: 2025,
        icons: [
            {
                "url": "https://arxiv.org/abs/2507.00775",
                "icon": projectLinkIcon.onlineLink
            },
            {
                "url": "https://physicalviswidget.github.io/",
                "icon": projectLinkIcon.website
            },
            {
                "url": "assets/pdf/Designing_Visualization_Widgets_for_Tangible_Data_Exploration_A_Systematic_Review.pdf",
                "icon": projectLinkIcon.pdf
            },
            {
                "url": "https://osf.io/vjw5e/",
                "icon": projectLinkIcon.osf
            }
        ],
        popups: [
            {
                icon: projectLinkIcon.bibtex,
                contentId: 14
            }
        ],
        study: [],
        selected: false
    },
    {
        id: 2,
        event: "ChinaVis",
        type: publicationTypeList.Poster,
        award: publicationAwardType.HonorableMention,
        title: "Visualization in Motion: Perception, Design, and User Experience",
        author: "<strong>Lijie Yao*</strong>",
        subtitle: "Posters of ChinaVis, July 2025, Hangzhou, China.",
        teaser: "assets/teaser/poster_VIM.jpg",
        year: 2025,
        icons: [
            {
                "url": "https://chinavis.org/2025/en/poster/",
                "icon": projectLinkIcon.onlineLink
            },
            {
                "url": "assets/pdf/Poster_VisInMotion.pdf",
                "icon": projectLinkIcon.pdf
            },
            {
                "url": "assets/poster/Poster_VisInMotion.pdf",
                "icon": projectLinkIcon.poster
            },
            {
                "url": "https://youtu.be/2R1zq9PKYyM",
                "icon": projectLinkIcon.video
            }
        ],
        popups: [
            {
                icon: projectLinkIcon.bibtex,
                contentId: 15
            }
        ],
        study: [],
        selected: false
    },
    {
        id: 3,
        event: "CG&A",
        type: publicationTypeList.FullPaper,
        title: "Situated Visualization in Motion",
        author: "<strong>Lijie Yao*</strong>",
        subtitle: "IEEE Computer Graphics and Applications, vol. 44, no. 6, pp. 142-150,2024 Nov.-Dec., doi: 10.1109/MCG.2024.3462129.",
        description: "We define visualization in motion and make several contributions to how to visualize and design situated visualizations in motion. In situated data visualization, the data is directly visualized near their data referent, i.e., the physical space, object, or person it refers to. Situated visualizations are often useful in contexts where the data referent or the viewer does not remain stationary but is in relative motion.  For example, a runner looks at visualizations from their fitness band while running. Reading visualizations in such scenarios might be impacted by motion factors. As such, understanding how to best design visualizations with motion factors is important. We define visualizations in motion as visual data representations used in contexts that exhibit relative motion between a viewer and an entire visualization. We propose a research agenda to understand what research opportunities and challenges are under visualization in motion. Next, we investigate (a) how motion factors can affect the reading accuracy of visualizations, (b) how to design and embed visualizations in motion in a real application scenario, and (c) the user experience and design trade-offs of visualization in motion through a case study.",
        teaser: "assets/teaser/visinmotion_logo.png",
        year: 2024,
        icons: [
            {
                "url": "https://doi.org/10.1109/MCG.2024.3462129",
                "icon": projectLinkIcon.doi
            },
            {
                "url": "https://hal.science/hal-04700820",
                "icon": projectLinkIcon.onlineLink
            },
            {
                "url": "assets/pdf/Situated_Visualization_in_Motion.pdf",
                "icon": projectLinkIcon.pdf
            }
        ],
        popups: [
            {
                icon: projectLinkIcon.bibtex,
                contentId: 1
            }
        ],
        study: [],
        selected: true
    },
    {
        id: 4,
        event: "TVCG",
        type: publicationTypeList.FullPaper,
        title: "User Experience of Visualizations in Motion: A Case Study and Design Considerations",
        author: "<strong>Lijie Yao*</strong>, Federica Bucchieri, Victoria McArthur, Anastasia Bezerianos, Petra Isenberg",
        subtitle: "IEEE Transactions on Visualization and Computer Graphics, 2025, 31(1), pp.174-184, doi: 1109/TVCG.2024.3456319.",
        description: "We present a systematic review, an empirical study, and a first set of considerations for designing visualizations in motion, derived from a concrete scenario in which these visualizations were used to support a primary task. In practice, when viewers are confronted with embedded visualizations, they often have to focus on a primary task and can only quickly glance at a visualization showing rich, often dynamically updated, information. As such, the visualizations must be designed so as not to distract from the primary task, while at the same time being readable and useful for aiding the primary task. For example, in games, players who are engaged in a battle have to look at their enemies but also read the remaining health of their own game character from the health bar over their character's head. Many trade-offs are possible in the design of embedded visualizations in such dynamic scenarios, which we explore in-depth in this paper with a focus on user experience. We use video games as an example of an application context with a rich existing set of visualizations in motion. We begin our work with a systematic review of in-game visualizations in motion. Next, we conduct an empirical user study to investigate how different embedded visualizations in motion designs impact user experience. We conclude with a set of considerations and trade-offs for designing visualizations in motion more broadly as derived from what we learned about video games. All supplemental materials of this paper are available at <a href=\"https://osf.io/3v8wm/\" target=\"_blank\">osf.io/3v8wm/</a>.",
        teaser: "assets/teaser/videogame.jpg",
        year: 2025,
        icons: [
            {
                "url": "https://doi.org/10.1109/TVCG.2024.3456319",
                "icon": projectLinkIcon.doi
            },
            {
                "url": "https://hal.science/hal-04700819",
                "icon": projectLinkIcon.onlineLink
            },
            {
                "url": "assets/pdf/User_Experience_of_Visualizations_in_Motion_A_Case_Study_and_Design_Considerations.pdf",
                "icon": projectLinkIcon.pdf
            },
            {
                "url": "https://www.youtube.com/watch?v=X9GOtQyXfx8",
                "icon": projectLinkIcon.video
            },
            {
                "url": "https://www.youtube.com/watch?v=7Y2cPfXGiAY&t=1237s",
                "icon": projectLinkIcon.video
            },
            {
                "url": "https://youtu.be/I7sD-IQlOZM",
                "icon": projectLinkIcon.video
            },
            {
                "url": "https://osf.io/3v8wm/",
                "icon": projectLinkIcon.osf
            },
            {
                "url": "https://github.com/lyao0219/RobotLife/",
                "icon": projectLinkIcon.github
            }
        ],
        popups: [
            {
                icon: projectLinkIcon.bibtex,
                contentId: 2
            }
        ],
        study: [],
        selected: true
    },
    {
        id: 5,
        event: "TVCG",
        type: publicationTypeList.FullPaper,
        title: "Designing for Visualization in Motion: Embedding Visualizations in Swimming Videos",
        author: "<strong>Lijie Yao*</strong>, Romain Vuillmot, Anastasia Bezerianos, Petra Isenberg",
        subtitle: "IEEE Transactions on Visualization and Computer Graphics, 2024, 30(3), pp.1821-1836, doi: 10.1109/tvcg.2023.3341990.",
        description: "We report on challenges and considerations for supporting design processes for visualizations in motion embedded in sports videos. We derive our insights from analyzing swimming race visualizations and motion-related data, building a technology probe, as well as a study with designers. Understanding how to design situated visualizations in motion is important for a variety of contexts. Competitive sports coverage, in particular, increasingly includes information on athlete or team statistics and records. Although moving visual representations attached to athletes or other targets are starting to appear, systematic investigations on how to best support their design process in the context of sports videos are still missing. Our work makes several contributions in identifying opportunities for visualizations to be added to swimming competition coverage but, most importantly, in identifying requirements and challenges for designing situated visualizations in motion. Our investigations include the analysis of a survey with swimming enthusiasts on their motion-related information needs, an ideation workshop to collect designs and elicit design challenges, the design of a technology probe that allows to create embedded visualizations in motion based on real data, and an evaluation with visualization designers that aimed to understand the benefits of designing directly on videos.",
        teaser: "assets/teaser/swimming.jpg",
        year: 2024,
        icons: [
            {
                "url": "https://doi.org/10.1109/tvcg.2023.3341990",
                "icon": projectLinkIcon.doi
            },
            {
                "url": "https://hal.science/hal-04364838",
                "icon": projectLinkIcon.onlineLink
            },
            {
                "url": "assets/pdf/Designing_for_Visualization_in_Motion_Embedding_Visualizations_in_Swimming_Videos.pdf",
                "icon": projectLinkIcon.pdf
            },
            {
                "url": "https://www.youtube.com/watch?v=lFf8sM52rMc",
                "icon": projectLinkIcon.video
            },
            {
                "url": "https://www.youtube.com/watch?v=7Y2cPfXGiAY&t=2011s",
                "icon": projectLinkIcon.video
            },
            {
                "url": "https://youtu.be/yC39NXBJiE8",
                "icon": projectLinkIcon.video
            },
            {
                "url": "https://www.replicabilitystamp.org/index.html#https-github-com-lyao0219-swimflow-git",
                "icon": projectLinkIcon.stamp
            },
            {
                "url": "https://motion.isenberg.cc/Swimming/index",
                "icon": projectLinkIcon.website
            },
            {
                "url": "https://osf.io/nxyr4/",
                "icon": projectLinkIcon.osf
            },
            {
                "url": "https://github.com/lyao0219/SwimFlow/",
                "icon": projectLinkIcon.github
            }
        ],
        popups: [
            {
                icon: projectLinkIcon.bibtex,
                contentId: 3
            }
        ],
        study: [],
        selected: true
    },
    {
        id: 6,
        event: "Thesis",
        type: publicationTypeList.Thesis,
        title: "Situated Visualization in Motion",
        author: "<strong>Lijie Yao*</strong>. \n Advisors: Petra Isenberg, Anastasia Bezerianos. \n Jury committee: Gilles Bally, Niklas Elmqvist, Morten Fjeld, Uta Hinrichs, Wendy Mackay, Melanie Tory.",
        subtitle: "Human-Computer Interaction [cs.HC]. Université Paris-Saclay, 2023. English. NNT: 2023UPASG093.",
        description: "The doctoral dissertation of Lijie Yao, which received the PhD Thesis Prize from AFIHM.",
        teaser: "assets/teaser/doctoralthesis.jpg",
        year: 2023,
        icons: [
            {
                "url": "https://theses.hal.science/tel-04413122",
                "icon": projectLinkIcon.onlineLink
            },
            {
                "url": "assets/pdf/Lijie_PhD_Thesis.pdf",
                "icon": projectLinkIcon.pdf
            },
            {
                "url": "https://youtu.be/0ED5iukU9tQ",
                "icon": projectLinkIcon.video
            }
        ],
        popups: [
            {
                icon: projectLinkIcon.bibtex,
                contentId: 13
            }
        ],
        study: [],
        selected: false
    },
    {
        id: 7,
        event: "TVCG",
        type: publicationTypeList.FullPaper,
        title: "Visualization in Motion: A Research Agenda and Two Evaluations",
        author: "<strong>Lijie Yao*</strong>, Anastasia Bezerianos, Romain Vuillmot, Petra Isenberg",
        subtitle: "IEEE Transactions on Visualization and Computer Graphics, 2022, 28(10), pp.3546-3562, doi: 10.1109/TVCG.2022.3184993.",
        description: "We contribute a research agenda for visualization in motion and two experiments to understand how well viewers can read data from moving visualizations. We define visualizations in motion as visual data representations that are used in contexts that exhibit relative motion between a viewer and an entire visualization. Sports analytics, video games, wearable devices, or data physicalizations are example contexts that involve different types of relative motion between a viewer and a visualization. To analyze the opportunities and challenges for designing visualization in motion, we show example scenarios and outline a first research agenda. Motivated primarily by the prevalence of and opportunities for visualizations in sports and video games we started to investigate a small aspect of our research agenda: the impact of two important characteristics of motion---speed and trajectory on a stationary viewer's ability to read data from moving donut and bar charts. We found that increasing speed and trajectory complexity did negatively affect the accuracy of reading values from the charts and that bar charts were more negatively impacted. In practice, however, this impact was small: both charts were still read fairly accurately.",
        teaser: "assets/teaser/visinmotion.jpg",
        year: 2022,
        icons: [
            {
                "url": "https://doi.org/10.1109/TVCG.2022.3184993",
                "icon": projectLinkIcon.doi
            },
            {
                "url": "https://hal.science/hal-03698837",
                "icon": projectLinkIcon.onlineLink
            },
            {
                "url": "assets/pdf/Vis_in_motion_A_Research_Agenda_and_Two_Evaluations.pdf",
                "icon": projectLinkIcon.pdf
            },
            {
                "url": "https://youtu.be/sIzRfNIsRV4",
                "icon": projectLinkIcon.video
            },
            {
                "url": "https://youtu.be/z9LsWYHSGHc",
                "icon": projectLinkIcon.video
            },
            {
                "url": "https://www.replicabilitystamp.org/index.html#https-gitlab-inria-fr-lyao-visinmotion",
                "icon": projectLinkIcon.stamp
            },
            {
                "url": "https://gitlab.inria.fr/lyao/visinmotion",
                "icon": projectLinkIcon.github
            }
        ],
        popups: [
            {
                icon: projectLinkIcon.bibtex,
                contentId: 4
            }
        ],
        study: [
            {
                "title": "STUDY 1",
                "logo1": {
                    "href": "https://osf.io/km3s2/",
                    "icon": "assets/img/osf.png"
                },
                "logo2": {
                    "href": "https://motion.isenberg.cc/study1_speed_donut/index.php?PROLIFIC=False",
                    "icon": "assets/img/study.png"
                }
            },
            {
                "title": "STUDY 2",
                "logo1": {
                    "href": "https://osf.io/t748d/",
                    "icon": "assets/img/osf.png"
                },
                "logo2": {
                    "href": "https://motion.isenberg.cc/study1_speed_bar/index.php?PROLIFIC=False",
                    "icon": "assets/img/study.png"
                }
            },
            {
                "title": "STUDY 3",
                "logo1": {
                    "href": "https://osf.io/9c4bz/",
                    "icon": "assets/img/osf.png"
                },
                "logo2": {
                    "href": "https://motion.isenberg.cc/study2_trajectory_donut/index.php?PROLIFIC=False",
                    "icon": "assets/img/study.png"
                }
            },
            {
                "title": "STUDY 4",
                "logo1": {
                    "href": "https://osf.io/9c4bz/",
                    "icon": "assets/img/osf.png"
                },
                "logo2": {
                    "href": "https://motion.isenberg.cc/study2_trajectory_bar/index.php?PROLIFIC=False",
                    "icon": "assets/img/study.png"
                }
            }
        ],
        selected: true
    },
    {
        id: 8,
        event: "IEEE VIS",
        type: publicationTypeList.ShortPaper,
        title: "Micro Visualizations on a Smartwatch: Assessing Reading Performance While Walking",
        author: "Fairouz Grioui*, Tanja Blascheck, <strong>Lijie Yao</strong>, Petra Isenberg",
        subtitle: "IEEE Visualization & Visual Analytics (IEEE VIS), October 2024, Florida, United States, doi: 10.1109/VIS55277.2024.00017.",
        description: "With two studies, we assess how different walking trajectories (straight line, circular, and infinity) and speeds (2km/h, 4km/h, and 6km/h) influence the accuracy and response time of participants reading micro visualizations on a smartwatch. We showed our participants common watch face micro visualizations including date, time, weather information, and four complications showing progress charts of fitness data. Our findings suggest that while walking trajectories did not significantly affect reading performance, overall walking activity, especially at high speeds, hurt reading accuracy and, to some extent, response time. Supplemental material is available at <a href=\"https://osf.io/u78s6/\" target=\"_blank\">osf.io/u78s6/</a>.\n                                    ",
        teaser: "assets/teaser/smartwatch.jpg",
        year: 2024,
        icons: [
            {
                "url": "https://doi.org/10.48550/arXiv.2407.17893",
                "icon": projectLinkIcon.doi
            },
            {
                "url": "https://hal.science/hal-04700821",
                "icon": projectLinkIcon.onlineLink
            },
            {
                "url": "assets/pdf/Micro_Visualizations_on_a_Smartwatch_Assessing_Reading_Performance_While_Walking.pdf",
                "icon": projectLinkIcon.pdf
            },
            {
                "url": "https://www.youtube.com/watch?v=2DE5LfUsIWA",
                "icon": projectLinkIcon.video
            },
            {
                "url": "https://www.youtube.com/watch?v=5O_zfoG4xOo&t=3931s",
                "icon": projectLinkIcon.video
            },
            {
                "url": "https://osf.io/u78s6/",
                "icon": projectLinkIcon.osf
            }
        ],
        popups: [
            {
                icon: projectLinkIcon.bibtex,
                contentId: 5
            }
        ],
        study: [],
        selected: false
    },
    {
        id: 9,
        event: "IEEE VIS",
        type: publicationTypeList.Proposal,
        title: "First-Person Visualizations for Outdoor Physical Activities: Challenges and Opportunities",
        author: "Charles Perin*, Tica Lin, <strong>Lijie Yao</strong>, Yalong Yang, Maxime Cordeil, Wesley Willett.",
        subtitle: "Selected proposals for workshops of the IEEE VIS, Oct. 2024, Florida, United States.",
        description: "The select proposals of the workshops at IEEE VIS 2022.",
        teaser: "assets/teaser/fpvis.jpg",
        year: 2024,
        icons: [
            {
                "url": "https://firstpersonvis.github.io/",
                "icon": projectLinkIcon.onlineLink
            },
            {
                "url": "assets/pdf/Charles_2024_VIS_Workshop_Proposal.pdf",
                "icon": projectLinkIcon.pdf
            },
            {
                "url": "https://youtu.be/4hNulDHo7Ak",
                "icon": projectLinkIcon.video
            }
        ],
        study: [],
        selected: false
    },
    {
        id: 10,
        event: "IEEE VIS",
        type: publicationTypeList.Proposal,
        title: "Situated Visualization in Motion",
        author: "<strong>Lijie Yao*</strong>.",
        subtitle: "Selected proposals for Doctoral Colloquium at IEEE VIS, Oct. 2022, Oklahoma City, United States.",
        description: "The select proposals of the doctoral colloquium at IEEE VIS 2022.",
        teaser: "assets/teaser/dc.jpg",
        year: 2022,
        icons: [
            {
                "url": "assets/pdf/Lijie_2022_VIS_Doctoral_Colloquium_Proposal.pdf",
                "icon": projectLinkIcon.pdf
            }
        ],
        study: [],
        selected: false
    },
    {
        id: 11,
        event: "IEEE VIS",
        type: publicationTypeList.Workshop,
        title: "Collecting Information Needs for Egocentric Visualizations while Running",
        author: "Ahmed Elshabasi*, <strong>Lijie Yao</strong>, Petra Isenberg, Charles Perin, Wesley Willett",
        subtitle: "IEEE VIS workshop on First-Person Visualizations for Outdoor Physical Activities (FPVis), October 2024, Florida, United States.",
        description: "We investigate research challenges and opportunities for visualization in motion during outdoor physical activities via an initial corpus of real-world recordings that pair egocentric video, biometrics, and think-aloud observations. With the increasing use of tracking and recording devices, such as smartwatches and head-mounted displays, more and more data are available in real-time about a person's activity and the context of the activity. However, not all data will be relevant all the time. Instead, athletes have information needs that change throughout their activity depending on the context and their performance. To address this challenge, we describe the collection of a diverse corpus of information needs paired with contextualizing audio, video, and sensor data. Next, we propose a first set of research challenges and design considerations that explore the difficulties of visualizing those real data needs in-context and demonstrate a prototype tool for browsing, aggregating, and analyzing this information. Our ultimate goal is to understand and support embedding visualizations into outdoor contexts with changing environments and varying data needs.\n                                    ",
        teaser: "assets/teaser/sports.jpg",
        year: 2024,
        icons: [
            {
                "url": "https://doi.org/10.48550/arXiv.2409.06426",
                "icon": projectLinkIcon.doi
            },
            {
                "url": "https://hal.science/hal-04700822",
                "icon": projectLinkIcon.onlineLink
            },
            {
                "url": "assets/pdf/Collecting_Information_Needs_for_Egocentric_Visualizations_while_Running.pdf",
                "icon": projectLinkIcon.pdf
            },
            {
                "url": "https://youtu.be/4hNulDHo7Ak&t=1908s",
                "icon": projectLinkIcon.video
            }
        ],
        popups: [
            {
                icon: projectLinkIcon.bibtex,
                contentId: 6
            }
        ],
        study: [],
        selected: false
    },
    {
        id: 12,
        event: "IEEE VIS",
        type: publicationTypeList.Workshop,
        title: "Envisioning Situated Visualizations of Environmental Footprints in an Urban Environment",
        author: "Yvonne Jansen, Federica Bucchieri, Pierre Dragicevic, Martin Hachet, Morgane Koval, Léana Petiot, Arnaud Prouzeau, Dieter Schmalstieg, <strong>Lijie Yao</strong>, Petra Isenberg*",
        subtitle: "IEEE VIS workshop on Visualization for Social Good (VIS4Good), October 2022, Oklahoma, United States.",
        description: "We present the results of a brainstorming exercise focused on how situated visualizations could be used to better understand the state of the environment and our personal behavioral impact on it. Specifically, we conducted a day long workshop in the French city of Bordeaux where we envisioned situated visualizations of urban environmental footprints. We explored the city and took photos and notes about possible situated visualizations of environmental footprints that could be embedded near places, people, or objects of interest. We found that our designs targeted four purposes and used four different methods that could be further explored to test situated visualizations for the protection of the environment.\n                                    ",
        teaser: "assets/teaser/vis4good.jpg",
        year: 2022,
        icons: [
            {
                "url": "https://doi.org/10.5281/zenodo.7053934",
                "icon": projectLinkIcon.doi
            },
            {
                "url": "https://hal.science/hal-03770857",
                "icon": projectLinkIcon.onlineLink
            },
            {
                "url": "assets/pdf/Environment_Footprint_by_Situated_Visualization.pdf",
                "icon": projectLinkIcon.pdf
            },
            {
                "url": "https://www.youtube.com/watch?v=6MEl4qZulSo&t=1620s",
                "icon": projectLinkIcon.video
            }
        ],
        study: [],
        popups: [
            {
                icon: projectLinkIcon.bibtex,
                contentId: 7
            }
        ],
        selected: false
    },
    {
        id: 13,
        event: "MobileHCI",
        type: publicationTypeList.Workshop,
        title: "Reflections on Visualization in Motion for Fitness Trackers",
        author: "Alaul Islam°, <strong>Lijie Yao*°</strong>, Anastasia Bezerianos, Tanja Blascheck, Tingying He, Bongshin Lee, Romain Vuillmot, Petra Isenberg",
        subtitle: "ACM MobileHCI workshop on New Trends in HCI and Sports (NTSPORT), September 2022, Vancouver, Canada.",
        description: "In this paper, we reflect on our past work towards understanding how to design visualizations for fitness trackers that are used in motion. We have coined the term \"visualization in motion\" for visualizations that are used in the presence of relative motion between a viewer and the visualization. Here, we describe how visualization in motion is relevant to sports scenarios. We also provide new data on current smartwatch visualizations for sports and discuss future challenges for visualizations in motion for fitness trackers.\n                                    ",
        teaser: "assets/teaser/smartwatch_vim.jpg",
        year: 2022,
        icons: [
            {
                "url": "https://doi.org/10.48550/arXiv.2409.06401",
                "icon": projectLinkIcon.doi
            },
            {
                "url": "https://dblp.org/rec/conf/mhci/YaoIBBHLVI22",
                "icon": projectLinkIcon.onlineLink
            },
            {
                "url": "assets/pdf/Relfections_on_Visualization_in_Motion_for_Fitness_Trackers.pdf",
                "icon": projectLinkIcon.pdf
            },
            {
                "url": "https://www.youtube.com/watch?v=7qy-dgjE4L4",
                "icon": projectLinkIcon.video
            }
        ],
        popups: [
            {
                icon: projectLinkIcon.bibtex,
                contentId: 8
            }
        ],
        study: [],
        selected: false
    },
    {
        id: 14,
        event: "EuroVis",
        type: publicationTypeList.Poster,
        title: "Situated Visualization in Motion for Video Games",
        author: "Federica Bucchieri*, <strong>Lijie Yao</strong>, Petra Isenberg",
        subtitle: " Posters of EuroVis, June 2022, Rome, Italy, doi: 10.2312/evp.20221119.",
        description: "We contribute a systematic review of situated visualizations in motion in the context of video games. Video games produce rich dynamic datasets during gameplay that are often visualized to help players succeed in a game. Often these visualizations are moving either because they are attached to moving game elements or due to camera changes. We want to understand to what extent this motion and contextual game factors impact how players can read these visualizations. In order to ground our work, we surveyed 160 visualizations in motion and their embeddings in the game world. Here, we report on our analysis and categorization of these visualizations.\n                                    ",
        teaser: "assets/teaser/vim_videogame.jpg",
        year: 2022,
        icons: [
            {
                "url": "https://doi.org/10.2312/evp.20221119",
                "icon": projectLinkIcon.doi
            },
            {
                "url": "https://hal.science/hal-03694019",
                "icon": projectLinkIcon.onlineLink
            },
            {
                "url": "assets/pdf/Situated_Visualization_in_Motion_for_Video_Games.pdf",
                "icon": projectLinkIcon.pdf
            },
            {
                "url": "assets/poster/Poster_Situated_Visualization_in_Motion_for_Video_games.pdf",
                "icon": "assets/img/poster.png"
            },
            {
                "url": "https://youtu.be/ANikqv1EHK8",
                "icon": projectLinkIcon.video
            }
        ],
        popups: [
            {
                icon: projectLinkIcon.bibtex,
                contentId: 9
            }
        ],
        study: [],
        selected: false
    },
    {
        id: 15,
        event: "JournéeVisu",
        type: publicationTypeList.Poster,
        title: "Situated Visualization in Motion for Swimming",
        author: "<strong>Lijie Yao*</strong>, Anastasia Bezerianos, Romain Vuillmot, Petra Isenberg",
        subtitle: " Posters of Journée Visu, June 2022, Bordeaux, France.",
        description: "Competitive sports coverage increasingly includes information on athlete or team statistics and records. Sports video coverage has traditionally embedded representations of this data in fixed locations on the screen, but more recently also attached representations to athletes or other targets in motion. These publicly used representations so far have been rather simple and systematic investigations of the research space of embedded visualizations in motion are still missing. Here we report on our preliminary research in the domain of professional and amateur swimming. We analyzed how visualizations are currently added to the coverage of Olympics swimming competitions and then plan to derive a design space for embedded data representations for swimming competitions. We are currently conducting a crowdsourced survey to explore which kind of swimming-related data general audiences are interested in, in order to identify opportunities for additional visualizations to be added to swimming competition coverage.",
        teaser: "assets/teaser/vim_swimming.jpg",
        year: 2022,
        icons: [
            {
                "url": "https://doi.org/10.48550/arXiv.2409.07695",
                "icon": projectLinkIcon.doi
            },
            {
                "url": "https://hal.science/hal-03700406",
                "icon": projectLinkIcon.onlineLink
            },
            {
                "url": "assets/pdf/Situated_Visualization_in_Motion_for_Swimming.pdf",
                "icon": projectLinkIcon.pdf
            },
            {
                "url": "assets/poster/Poster_Situated_Visualization_in_Motion_Swimming.pdf",
                "icon": projectLinkIcon.poster
            }
        ],
        study: [],
        popups: [
            {
                icon: projectLinkIcon.bibtex,
                contentId: 10
            }
        ],
        selected: false
    },
    {
        id: 16,
        event: "JournéeVisu",
        type: publicationTypeList.Poster,
        title: "Visualization in Motion in Video Games for Different Types of Data",
        author: "Federica Bucchieri*, <strong>Lijie Yao</strong>, Petra Isenberg",
        subtitle: " Posters of Journée Visu, June 2022, Bordeaux, France.",
        description: "We contribute an analysis of situated visualizations in motion in video games for different types of data, with a focus on quantitative and categorical data representations. Video games convey a lot of data to players, to help them succeed in the game. These visualizations frequently move across the screen due to camera changes or because the game elements themselves move. Our ultimate goal is to understand how motion factors affect visualization readability in video games and subsequently the players' performance in the game. We started our work by surveying the characteristics of how motion currently influences which kind of data representations in video games. We conducted a systematic review of 160 visualizations in motion in video games and extracted patterns and considerations regarding was what, and how visualizations currently exhibit motion factors in video games.\n                                    ",
        teaser: "assets/teaser/vim_gamedata.jpg",
        year: 2022,
        icons: [
            {
                "url": "https://doi.org/10.48550/arXiv.2409.07696",
                "icon": projectLinkIcon.doi
            },
            {
                "url": "https://hal.science/hal-03700418",
                "icon": projectLinkIcon.onlineLink
            },
            {
                "url": "assets/pdf/Visualization_in_Motion_for_Video_Games_Data_Type.pdf",
                "icon": projectLinkIcon.pdf
            },
            {
                "url": "assets/poster/Poster_Situated_Visualization_in_Motion_Game_Data.pdf",
                "icon": projectLinkIcon.poster
            }
        ],
        study: [],
        popups: [
            {
                icon: projectLinkIcon.bibtex,
                contentId: 11
            }
        ],
        selected: false
    },
    {
        id: 17,
        event: "IEEE VIS",
        type: publicationTypeList.Poster,
        title: "Situated Visualization in Motion",
        author: "<strong>Lijie Yao*</strong>, Anastasia Bezerianos, Petra Isenberg",
        subtitle: "Posters of IEEE VIS, October 2020, Salt Lake City, United States.",
        description: "We contribute a first design space on visualizations in motion and the design of a pilot study we plan to run in the fall. Visualizations can be useful in contexts where either the observation is in motion or the whole visualization is moving at various speeds. Imagine, for example, displays attached to an athlete or animal that show data about the wearer – for example, captured from a fitness tracking band; or a visualization attached to a moving object such as a vehicle or a soccer ball. The ultimate goal of our research is to inform the design of visualizations under motion.\n                                    ",
        teaser: "assets/teaser/vim.jpg",
        year: 2020,
        icons: [
            {
                "url": "https://doi.org/10.48550/arXiv.2409.07005",
                "icon": projectLinkIcon.doi
            },
            {
                "url": "https://hal.science/hal-02946587",
                "icon": projectLinkIcon.onlineLink
            },
            {
                "url": "assets/pdf/Situated_Visualization_in_Motion_Poster.pdf",
                "icon": projectLinkIcon.pdf
            },
            {
                "url": "https://www.youtube.com/watch?v=gV4KobSoiy0",
                "icon": projectLinkIcon.video
            }
        ],
        study: [],
        popups: [
            {
                icon: projectLinkIcon.bibtex,
                contentId: 12
            }
        ],
        selected: false
    },
    {
        id: 18,
        event: "IEEE VIS",
        type: publicationTypeList.Workshop,
        title: "Early Exploration into AI-Assisted Visual Analytics for Dynamic Videos",
        author: "Qi Guo, Junyi Li, Jiayi Hong, <strong>Lijie Yao*</strong>",
        subtitle: "IEEE VIS workshop on GenAI, Agents, and the Future of VIS (VISxGenAI), November 2025, Vienna, Austria. To appear.",
        description: "We present a preliminary investigation into the capabilities of current large language models (LLMs), i.e., ChatGPT and Gemini, in supporting visual analytics tasks for videos containing dynamically changing information. Videos are inherently multimodal, combining visual frames, audio narration, and sometimes text---often with inconsistencies or redundancies across channels---which poses challenges for reliable data extraction. While recent advances in video understanding have improved general-purpose AI performance, relatively little work has explored how generative AI can extract, prepare, and visualize data from videos through prompts, particularly where multimodal conflicts, dynamic updates, and moving entities are involved. To explore this space, we first categorize information-bearing videos along four dimensions: data type, data dynamics, visualization presence, and audio-visual alignment. We then apply LLMs to extract and structure information from representative video samples to support downstream visualization. We conclude with reflections and outline a research agenda for AI-assisted video-based visual analytics. Our OSF repository is at https://osf.io/ygn4c/.\n                                    ",
        teaser: "assets/teaser/eeAIvideos.jpg",
        year: 2025,
        icons: [
            {
                "url": "assets/pdf/Early_Exploration_into_AI-Assisted_Visual_Analytics_for_Dynamic_Videos.pdf",
                "icon": projectLinkIcon.pdf
            }
        ],
        study: [],
        popups: [
            {
                icon: projectLinkIcon.bibtex,
                contentId: 16
            }
        ],
        selected: false
    },
    {
        id: 19,
        event: "IEEE VIS",
        type: publicationTypeList.Workshop,
        title: "Structured AI Agents for Reliable Visualization Report Generation",
        author: "Junhao Zhao, <strong>Lijie Yao*</strong>",
        subtitle: "IEEE VIS workshop on GenAI, Agents, and the Future of VIS (VISxGenAI), November 2025, Vienna, Austria. To appear.",
        description: "We introduce Structured AI Agents to tackle instability and hallucination in AI agent–based data visualization. AI agents promise faster, more accessible visualization reporting, but current methods remain inconsistent and struggle to deliver reliable results. We set multiple sub-tasks and form a structured pipeline, where each sub-task unit produces standardized JSON outputs encoding executable and deterministic code. Our approach enforces correctness in quantitative results, reduces hallucinations, and ensures reliable visualizations. In our final report, we demonstrate that the modular and verifiable outputs achieve strong reliability and maintain accuracy in visualization report generation.",
        teaser: "assets/teaser/structuredAIagents.jpg",
        year: 2025,
        icons: [
            {
                "url": "assets/pdf/Structured_AI_Agents_for_Reliable_Visualization_Report_Generation.pdf",
                "icon": projectLinkIcon.pdf
            }
        ],
        study: [],
        popups: [
            {
                icon: projectLinkIcon.bibtex,
                contentId: 17
            }
        ],
        selected: false
    },
    {
        id: 20,
        event: "IEEE VIS",
        type: publicationTypeList.Workshop,
        title: "Input Physicalization in Practice: An Instructional Walkthrough with Visualization Novices",
        author: "<strong>Lijie Yao*</strong>, Yuming Zhang, Yu Liu",
        subtitle: "IEEE VIS workshop on Input Visualization (InputVis), November 2025, Vienna, Austria. To appear.",
        description: "We conduct an instructional walkthrough with undergraduate students who have limited to no visualization knowledge and create three physicalizations with personal data input. While physicalization is widely recognized as a tangible , intuitive, and engaging form of data representation, its design and construction remain challenging, especially for novices who lack basic visualization knowledge. To explore these challenges in practice, we conducted an instructional walkthrough with undergraduate students who have limited to no visualization expertise. A group of students first elicited possible personal data, which was then filtered and categorized under supervision. Next, one lead student discussed with the supervisors to determine the reasonable data items to design with, as well as the possible data input methods. After which, the student individually modeled and fabricated the physicalizations that have corresponding data input mechanisms , and used these physicalizations to complete a period of data input. Our walkthrough revealed insights into input physicalization in connection with visualization novices and education, as well as the research gaps between observed novice challenges with input visualization. Supplemental materials and demo videos are available at https://osf.io/8quv4/.",
        teaser: "assets/teaser/inputPhysicalization.jpg",
        year: 2025,
        icons: [
            {
                "url": "assets/pdf/Input_Physicalization_in_Practice_An_Instructional_Walkthrough_with_Visualization_Novices.pdf",
                "icon": projectLinkIcon.pdf
            }
        ],
        study: [],
        popups: [
            {
                icon: projectLinkIcon.bibtex,
                contentId: 18
            }
        ],
        selected: false
    },
    {
        id: 21,
        event: "VINCI",
        type: publicationTypeList.FullPaper,
        title: "Touch, Sound, and Space: Exploring Immersive Music Interaction through AI-Generated Environments",
        author: "Wanfang Xu, Jifan Yang, Fengwen Zhang, Yu Lu, <strong>Lijie Yao</strong>, Le Liu, Lingyun Yu*",
        subtitle: "International Symposium on Visual Information Communication and Interaction (VINCI), December 2025, Linz, Austria. To appear.",
        description: "We introduce an interactive music system powered by AI-generated content (AIGC) that enables users to engage with music through multimodal interactions involving touch, sound, and spatial immersion. Motivated by the desire to enhance engagement and emotional connection with music, our system enables users to co-create and interact with musical content. Users upload a song and a descriptive text prompt, from which the system generates 3D visuals. During playback, users can embed their own audio inputs and trigger responsive visual effects such as color-driven point clouds using tangible controls. To explore how spatial scale and embodiment shape user experience, we implement the system across three increasing spatial scale and embodiment: (1) a handheld AR music box, (2) a table-sized stage box, and (3) a fully immersive VR environment. Through a user study, we investigate how different levels of immersion and interaction influence user engagement, emotional response, and sense of presence. Our findings demonstrate the potential of combining AIGC with embodied interaction to enrich creative expression and enhance immersive musical experiences.",
        teaser: "assets/teaser/touchSoundSpace.jpg",
        year: 2025,
        icons: [
            {
                "url": "assets/pdf/Touch_Sound_and_Space_Exploring_Immersive_Music_Interaction_through_AI-Generated_Environments.pdf",
                "icon": projectLinkIcon.pdf
            }
        ],
        study: [],
        popups: [
            {
                icon: projectLinkIcon.bibtex,
                contentId: 19
            }
        ],
        selected: false
    }   
]
